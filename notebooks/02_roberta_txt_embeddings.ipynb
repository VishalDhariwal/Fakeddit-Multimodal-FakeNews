{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SymsCDqEjatr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/df_train_full.csv')\n",
        "df_test = pd.read_csv('/content/df_test_clean.csv')"
      ],
      "metadata": {
        "id": "CIIs7GN2mZSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.shape)\n",
        "print(df_test.shape)\n"
      ],
      "metadata": {
        "id": "vEOjVdzRmm6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "mDcrYYLcmwEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['hasImage'].value_counts()"
      ],
      "metadata": {
        "id": "z9hEFPvTnaw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_num = df_train.select_dtypes(include='number')\n"
      ],
      "metadata": {
        "id": "nijFFnV6oZY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_num.corr()"
      ],
      "metadata": {
        "id": "lCzrIH5doq92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def extract_embeddings(df):\n",
        "    texts = df['clean_title'].tolist()\n",
        "    batch_size = 16\n",
        "    features = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        encoded = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        features.append(cls_embeddings)\n",
        "\n",
        "    return np.vstack(features)\n",
        "\n",
        "X_train_bert = extract_embeddings(df_train)\n",
        "X_test_bert = extract_embeddings(df_test)"
      ],
      "metadata": {
        "id": "iXmFmKoZqQkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the folder path\n",
        "folder_path = '/content/drive/MyDrive/multimodel_dataset_extracted'\n",
        "\n",
        "# 3. Define the full file paths\n",
        "train_save_path = os.path.join(folder_path, 'X_text_emb_train.npy')\n",
        "test_save_path = os.path.join(folder_path, 'X_text_emb_test.npy')\n",
        "\n",
        "# 4. Save the NumPy arrays\n",
        "# Assuming X_train_bert and X_test_bert are your embedding arrays\n",
        "np.save(train_save_path, X_train_bert)\n",
        "np.save(test_save_path, X_test_bert)\n",
        "\n",
        "print(f\"✅ Training embeddings saved successfully to: {train_save_path}\")\n",
        "print(f\"✅ Testing embeddings saved successfully to: {test_save_path}\")"
      ],
      "metadata": {
        "id": "nGwd-LvZsB6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_delete = ['clean_title','author','subreddit','domain', 'created_utc', 'hasImage', 'id', 'image_url', 'linked_submission_id', 'title']\n",
        "df_train_mod = df_train.drop(columns=cols_delete)\n",
        "df_test_mod = df_test.drop(columns=cols_delete)"
      ],
      "metadata": {
        "id": "vVJbGSyJtOOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_mod = df_test.drop(columns=cols_delete)\n",
        "df_train_mod.head()"
      ],
      "metadata": {
        "id": "0-cDKMVrt69k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_mod.iloc[:,:3]"
      ],
      "metadata": {
        "id": "JYD7xU5iuXWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "print(\"Skewness:\", skew(df_train_mod['num_comments'].dropna()))\n"
      ],
      "metadata": {
        "id": "Sj07FbCqvBKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Apply log1p to reduce skewness\n",
        "df_train_mod['num_comments_log'] = np.log1p(df_train_mod['num_comments'])\n",
        "\n",
        "# 2. (Optional) Scale the log-transformed data\n",
        "scaler = StandardScaler()\n",
        "df_train_mod['num_comments_scaled'] = scaler.fit_transform(df_train_mod[['num_comments_log']])\n",
        "\n"
      ],
      "metadata": {
        "id": "VpfupoTuvYaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_mod['num_comments_log'] = np.log1p(df_test_mod['num_comments'])\n",
        "df_test_mod['num_comments_scaled'] = scaler.transform(df_test_mod[['num_comments_log']])"
      ],
      "metadata": {
        "id": "va13k-UzveAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "print(\"Skewness:\", skew(df_train_mod['score'].dropna()))\n"
      ],
      "metadata": {
        "id": "Z4Jh2-h_vlSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Apply log1p to reduce skewness\n",
        "df_train_mod['score_log'] = np.log1p(df_train_mod['score'])\n",
        "\n",
        "# 2. (Optional) Scale the log-transformed data\n",
        "scaler = StandardScaler()\n",
        "df_train_mod['score_scaled'] = scaler.fit_transform(df_train_mod[['score_log']])\n",
        "\n"
      ],
      "metadata": {
        "id": "9JqVgO0Mvs3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_test_mod['score_log'] = np.log1p(df_test_mod['score'])\n",
        "\n",
        "# 2. (Optional) Scale the log-transformed data\n",
        "scaler = StandardScaler()\n",
        "df_test_mod['num_comments_scaled'] = scaler.fit_transform(df_test_mod[['score_log']])"
      ],
      "metadata": {
        "id": "2XRZHpIUxtM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_mod['score_scaled'] = scaler.transform(df_test_mod[['score']])\n"
      ],
      "metadata": {
        "id": "esnrVAvhvyBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_mod = df_test_mod.drop(columns=['score', 'num_comments'])\n",
        "df_train_mod = df_train_mod.drop(columns=['score', 'num_comments'])"
      ],
      "metadata": {
        "id": "mcsApBQ_v1-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_mod.head()"
      ],
      "metadata": {
        "id": "t-IIqzv9v_8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8KMQAS8wjFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}