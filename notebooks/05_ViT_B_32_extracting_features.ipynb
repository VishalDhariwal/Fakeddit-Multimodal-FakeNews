{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W49nl4rcVJZE",
        "outputId": "654da778-2c00-4b2a-de4a-bbb9261a399e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install clip-anytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LBvh77SVV3g",
        "outputId": "767827a3-0cea-42c2-a263-c0a9cb16e089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clip-anytorch\n",
            "  Downloading clip_anytorch-2.6.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting ftfy (from clip-anytorch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip-anytorch) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip-anytorch) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip-anytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip-anytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip-anytorch) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip-anytorch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip-anytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip-anytorch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip-anytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip-anytorch) (3.0.3)\n",
            "Downloading clip_anytorch-2.6.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, clip-anytorch\n",
            "Successfully installed clip-anytorch-2.6.0 ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O40d5tNLVkbc",
        "outputId": "e010f821-1691-4753-8cc2-72cda16dc4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " clip_embeddings_multimodal\n",
            " Code\n",
            "'Colab Notebooks'\n",
            "'Copy of bertmodelpreview.ipynb'\n",
            "'Fake News Detection: LAIR Dataset Project (1).gdoc'\n",
            "'Fake News Detection: LAIR Dataset Project.gdoc'\n",
            "'Fake News Detection Project Guidance.gdoc'\n",
            " LibraryManagement\n",
            " lightgbm_model.pkl\n",
            " ML-Project\n",
            " results\n",
            " results_local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/clip_embeddings_multimodal/\n"
      ],
      "metadata": {
        "id": "-AXIYhOaWEma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXhAeeVkU5qy",
        "outputId": "0cb32f6c-972b-4dca-d8f0-cc9a3e996110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Train shape: (564000, 16)\n",
            "âœ… Test shape:  (59319, 16)\n",
            "âœ… Split into halves: 282000 + 282000\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 354M/354M [00:04<00:00, 72.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing 282000 URLs with a 15-second timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   1%|          | 3183/282000 [00:19<22:17, 208.48it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 282000/282000 [23:08<00:00, 203.15it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Verification ---\n",
            "Original DataFrame shape: (282000, 16)\n",
            "Cleaned DataFrame shape:  (81705, 16)\n",
            "Image features shape:     (81705, 512)\n",
            "\n",
            "âœ… Saved Train Half 1 embeddings and cleaned CSV.\n",
            "\n",
            "Processing 282000 URLs with a 15-second timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing images:   0%|          | 0/282000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 282000/282000 [21:58<00:00, 213.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Verification ---\n",
            "Original DataFrame shape: (282000, 16)\n",
            "Cleaned DataFrame shape:  (78481, 16)\n",
            "Image features shape:     (78481, 512)\n",
            "\n",
            "âœ… Saved Train Half 2 embeddings and cleaned CSV.\n",
            "\n",
            "Processing 59319 URLs with a 15-second timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images:   2%|â–         | 963/59319 [00:04<04:06, 236.36it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59319/59319 [04:58<00:00, 198.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Verification ---\n",
            "Original DataFrame shape: (59319, 16)\n",
            "Cleaned DataFrame shape:  (16696, 16)\n",
            "Image features shape:     (16696, 512)\n",
            "\n",
            "âœ… Saved Test embeddings and cleaned CSV.\n",
            "\n",
            "ğŸ‰ All CLIP image embeddings processed and saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import clip\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- 2. Paths ---\n",
        "folder_path = '/content/drive/MyDrive/clip_embeddings_multimodal'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "train_path = '/content/drive/MyDrive/ML-Project/multimodal_train.tsv'\n",
        "test_path = '/content/drive/MyDrive/ML-Project/multimodal_test_public.tsv'\n",
        "\n",
        "# --- 3. Load Datasets ---\n",
        "df_train = pd.read_csv(train_path, delimiter='\\t')\n",
        "df_test = pd.read_csv(test_path, delimiter='\\t')\n",
        "\n",
        "print(f\"âœ… Train shape: {df_train.shape}\")\n",
        "print(f\"âœ… Test shape:  {df_test.shape}\")\n",
        "\n",
        "# Split train dataset into 2 halves\n",
        "mid_idx = len(df_train) // 2\n",
        "df_train_half_1 = df_train.iloc[:mid_idx].reset_index(drop=True)\n",
        "df_train_half_2 = df_train.iloc[mid_idx:].reset_index(drop=True)\n",
        "print(f\"âœ… Split into halves: {len(df_train_half_1)} + {len(df_train_half_2)}\")\n",
        "\n",
        "# --- 4. Setup CLIP ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.eval()\n",
        "\n",
        "# --- 5. Helper: download & preprocess ---\n",
        "def _download_and_preprocess_image(data, timeout):\n",
        "    index, url = data\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        tensor = preprocess(img)\n",
        "        return index, tensor\n",
        "    except Exception:\n",
        "        return index, None\n",
        "\n",
        "# --- 6. Main feature extraction function ---\n",
        "def extract_image_features(df, timeout_seconds=20):\n",
        "    print(f\"\\nProcessing {len(df)} URLs with a {timeout_seconds}-second timeout...\")\n",
        "\n",
        "    urls = df['image_url'].tolist()\n",
        "    url_data = list(zip(df.index, urls))\n",
        "\n",
        "    batch_size = 64\n",
        "    all_features = []\n",
        "    successful_indices = []\n",
        "\n",
        "    func = lambda data: _download_and_preprocess_image(data, timeout=timeout_seconds)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
        "        results_gen = executor.map(func, url_data)\n",
        "        batch = []\n",
        "        for index, tensor in tqdm(results_gen, total=len(urls), desc=\"Processing images\"):\n",
        "            if tensor is not None:\n",
        "                successful_indices.append(index)\n",
        "                batch.append(tensor)\n",
        "\n",
        "            if len(batch) == batch_size:\n",
        "                input_batch = torch.stack(batch).to(device)\n",
        "                with torch.no_grad():\n",
        "                    features = model.encode_image(input_batch)\n",
        "                    features = features / features.norm(dim=-1, keepdim=True)\n",
        "                all_features.append(features.cpu().numpy())\n",
        "                batch = []\n",
        "\n",
        "        # Process remainder\n",
        "        if len(batch) > 0:\n",
        "            input_batch = torch.stack(batch).to(device)\n",
        "            with torch.no_grad():\n",
        "                features = model.encode_image(input_batch)\n",
        "                features = features / features.norm(dim=-1, keepdim=True)\n",
        "            all_features.append(features.cpu().numpy())\n",
        "\n",
        "    if not all_features:\n",
        "        print(\"âš ï¸ No images processed successfully.\")\n",
        "        return pd.DataFrame(), np.array([])\n",
        "\n",
        "    X_img = np.vstack(all_features)\n",
        "    df_clean = df.loc[successful_indices].reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n--- Verification ---\")\n",
        "    print(f\"Original DataFrame shape: {df.shape}\")\n",
        "    print(f\"Cleaned DataFrame shape:  {df_clean.shape}\")\n",
        "    print(f\"Image features shape:     {X_img.shape}\")\n",
        "\n",
        "    return df_clean, X_img\n",
        "\n",
        "# --- 7. Process & Save ---\n",
        "\n",
        "# 7.1 Train Half 1\n",
        "df_train_half1_clean, X_img_half1 = extract_image_features(df_train_half_1, timeout_seconds=15)\n",
        "np.save(os.path.join(folder_path, \"X_img_train_half1.npy\"), X_img_half1)\n",
        "df_train_half1_clean.to_csv(os.path.join(folder_path, \"df_train_half1_clean.csv\"), index=False)\n",
        "print(\"\\nâœ… Saved Train Half 1 embeddings and cleaned CSV.\")\n",
        "\n",
        "# 7.2 Train Half 2\n",
        "df_train_half2_clean, X_img_half2 = extract_image_features(df_train_half_2, timeout_seconds=15)\n",
        "np.save(os.path.join(folder_path, \"X_img_train_half2.npy\"), X_img_half2)\n",
        "df_train_half2_clean.to_csv(os.path.join(folder_path, \"df_train_half2_clean.csv\"), index=False)\n",
        "print(\"\\nâœ… Saved Train Half 2 embeddings and cleaned CSV.\")\n",
        "\n",
        "# 7.3 Test Set\n",
        "df_test_clean, X_img_test = extract_image_features(df_test, timeout_seconds=15)\n",
        "np.save(os.path.join(folder_path, \"X_img_test.npy\"), X_img_test)\n",
        "df_test_clean.to_csv(os.path.join(folder_path, \"df_test_clean.csv\"), index=False)\n",
        "print(\"\\nâœ… Saved Test embeddings and cleaned CSV.\")\n",
        "\n",
        "print(\"\\nğŸ‰ All CLIP image embeddings processed and saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import clip\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- 2. Paths ---\n",
        "folder_path = '/content/drive/MyDrive/clip_embeddings_multimodal'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# --- 3. Load cleaned DataFrames (from image embedding extraction) ---\n",
        "df_train_half1_clean = pd.read_csv(os.path.join(folder_path, \"/content/drive/MyDrive/clip_embeddings_multimodal/df_train_half1_clean.csv\"))\n",
        "df_train_half2_clean = pd.read_csv(os.path.join(folder_path, \"/content/drive/MyDrive/clip_embeddings_multimodal/df_train_half2_clean.csv\"))\n",
        "df_test_clean = pd.read_csv(os.path.join(folder_path, \"/content/drive/MyDrive/clip_embeddings_multimodal/df_test_clean.csv\"))\n",
        "\n",
        "# Combine both train halves\n",
        "df_train_clean = pd.concat([df_train_half1_clean, df_train_half2_clean], ignore_index=True)\n",
        "print(\"âœ… Loaded and combined cleaned DataFrames corresponding to image embeddings:\")\n",
        "print(f\"Train combined shape: {df_train_clean.shape}\")\n",
        "print(f\"Test shape:           {df_test_clean.shape}\")\n",
        "\n",
        "# --- 4. Setup CLIP ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.eval()\n",
        "\n",
        "# --- 5. Text Embedding Function ---\n",
        "def extract_text_features(df, text_column, batch_size=64):\n",
        "    \"\"\"\n",
        "    Extracts CLIP text embeddings for a given column in a DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"\\nExtracting text embeddings from column: '{text_column}' ({len(df)} rows)\")\n",
        "    texts = df[text_column].astype(str).tolist()\n",
        "    all_features = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding text\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        tokens = clip.tokenize(batch_texts, truncate=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            text_features = model.encode_text(tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        all_features.append(text_features.cpu().numpy())\n",
        "\n",
        "    X_txt = np.vstack(all_features)\n",
        "    print(f\"âœ… Extracted text feature shape: {X_txt.shape}\")\n",
        "    return df, X_txt\n",
        "\n",
        "# --- 6. Process & Save ---\n",
        "\n",
        "# ğŸŸ¦ Training (combined halves)\n",
        "df_train_clean, X_txt_train = extract_text_features(df_train_clean, text_column='clean_title')\n",
        "np.save(os.path.join(folder_path, \"X_txt_train.npy\"), X_txt_train)\n",
        "df_train_clean.to_csv(os.path.join(folder_path, \"df_txt_train_clean.csv\"), index=False)\n",
        "print(\"\\nâœ… Saved combined training text embeddings and cleaned CSV.\")\n",
        "\n",
        "# ğŸŸ¨ Test set\n",
        "df_test_clean, X_txt_test = extract_text_features(df_test_clean, text_column='clean_title')\n",
        "np.save(os.path.join(folder_path, \"X_txt_test.npy\"), X_txt_test)\n",
        "df_test_clean.to_csv(os.path.join(folder_path, \"df_txt_test_clean.csv\"), index=False)\n",
        "print(\"\\nâœ… Saved test text embeddings and cleaned CSV.\")\n",
        "\n",
        "print(\"\\nğŸ‰ All CLIP text embeddings extracted successfully for image-matched rows!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UIWfKGLVU70",
        "outputId": "c36a664f-4ecd-42a9-9284-df026c48901a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Loaded and combined cleaned DataFrames corresponding to image embeddings:\n",
            "Train combined shape: (160186, 16)\n",
            "Test shape:           (16696, 16)\n",
            "Using device: cuda\n",
            "\n",
            "Extracting text embeddings from column: 'clean_title' (160186 rows)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2503/2503 [01:51<00:00, 22.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extracted text feature shape: (160186, 512)\n",
            "\n",
            "âœ… Saved combined training text embeddings and cleaned CSV.\n",
            "\n",
            "Extracting text embeddings from column: 'clean_title' (16696 rows)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261/261 [00:11<00:00, 22.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extracted text feature shape: (16696, 512)\n",
            "\n",
            "âœ… Saved test text embeddings and cleaned CSV.\n",
            "\n",
            "ğŸ‰ All CLIP text embeddings extracted successfully for image-matched rows!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- 2. Paths ---\n",
        "folder_path = '/content/drive/MyDrive/clip_embeddings_multimodal'\n",
        "\n",
        "# --- 3. Load image halves ---\n",
        "img_half1_path = os.path.join(folder_path, \"X_img_train_half1.npy\")\n",
        "img_half2_path = os.path.join(folder_path, \"X_img_train_half2.npy\")\n",
        "df_half1_path = os.path.join(folder_path, \"df_train_half1_clean.csv\")\n",
        "df_half2_path = os.path.join(folder_path, \"df_train_half2_clean.csv\")\n",
        "\n",
        "print(\"Loading halves...\")\n",
        "X_img_half1 = np.load(img_half1_path)\n",
        "X_img_half2 = np.load(img_half2_path)\n",
        "df_train_half1 = pd.read_csv(df_half1_path)\n",
        "df_train_half2 = pd.read_csv(df_half2_path)\n",
        "\n",
        "# --- 4. Combine halves ---\n",
        "print(\"\\nCombining...\")\n",
        "X_img_train = np.vstack([X_img_half1, X_img_half2])\n",
        "df_train_clean = pd.concat([df_train_half1, df_train_half2], ignore_index=True)\n",
        "\n",
        "# --- 5. Save combined files ---\n",
        "np.save(os.path.join(folder_path, \"X_img_train.npy\"), X_img_train)\n",
        "df_train_clean.to_csv(os.path.join(folder_path, \"df_train_clean.csv\"), index=False)\n",
        "\n",
        "print(\"\\nâœ… Combined image embeddings and cleaned DataFrames saved successfully!\")\n",
        "print(f\"Final image embedding shape: {X_img_train.shape}\")\n",
        "print(f\"Final DataFrame shape:       {df_train_clean.shape}\")\n",
        "\n",
        "# --- 6. Optional: Verify alignment ---\n",
        "assert len(df_train_clean) == len(X_img_train), \"âš ï¸ Length mismatch between embeddings and DataFrame!\"\n",
        "print(\"\\nâœ… Alignment verified: same number of rows in DataFrame and embeddings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CPPzsaTqagO",
        "outputId": "d51d12bf-4d93-41e9-d6bc-a0a5913d113e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loading halves...\n",
            "\n",
            "Combining...\n",
            "\n",
            "âœ… Combined image embeddings and cleaned DataFrames saved successfully!\n",
            "Final image embedding shape: (160186, 512)\n",
            "Final DataFrame shape:       (160186, 16)\n",
            "\n",
            "âœ… Alignment verified: same number of rows in DataFrame and embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- 2. Paths ---\n",
        "old_folder = '/content/drive/MyDrive/clip_embeddings_multimodal'\n",
        "new_folder = '/content/drive/MyDrive/ML-Project/clip_embeddings_multimodal'\n",
        "os.makedirs(new_folder, exist_ok=True)\n",
        "\n",
        "# --- 3. Load and combine TRAIN image halves ---\n",
        "print(\"ğŸ“¦ Loading and combining TRAIN image halves...\")\n",
        "X_img_half1 = np.load(os.path.join(old_folder, \"X_img_train_half1.npy\"))\n",
        "X_img_half2 = np.load(os.path.join(old_folder, \"X_img_train_half2.npy\"))\n",
        "X_img_train = np.vstack([X_img_half1, X_img_half2])\n",
        "\n",
        "df_train_half1 = pd.read_csv(os.path.join(old_folder, \"df_train_half1_clean.csv\"))\n",
        "df_train_half2 = pd.read_csv(os.path.join(old_folder, \"df_train_half2_clean.csv\"))\n",
        "df_train_clean = pd.concat([df_train_half1, df_train_half2], ignore_index=True)\n",
        "\n",
        "print(f\"âœ… Combined train image embeddings: {X_img_train.shape}\")\n",
        "print(f\"âœ… Combined train DataFrame: {df_train_clean.shape}\")\n",
        "\n",
        "# --- 4. Load TRAIN text embeddings ---\n",
        "print(\"\\nğŸ“¦ Loading TRAIN text embeddings...\")\n",
        "txt_half1_path = os.path.join(old_folder, \"X_txt_train_half1.npy\")\n",
        "txt_half2_path = os.path.join(old_folder, \"X_txt_train_half2.npy\")\n",
        "txt_full_path = os.path.join(old_folder, \"X_txt_train.npy\")\n",
        "\n",
        "if os.path.exists(txt_full_path):\n",
        "    print(\"âœ… Found single combined text embedding file.\")\n",
        "    X_txt_train = np.load(txt_full_path)\n",
        "else:\n",
        "    print(\"â„¹ï¸ Half text embeddings found â€” combining...\")\n",
        "    X_txt_half1 = np.load(txt_half1_path)\n",
        "    X_txt_half2 = np.load(txt_half2_path)\n",
        "    X_txt_train = np.vstack([X_txt_half1, X_txt_half2])\n",
        "\n",
        "# --- Sanity check ---\n",
        "assert len(df_train_clean) == len(X_img_train) == len(X_txt_train), \\\n",
        "    \"âŒ Mismatch between training data lengths!\"\n",
        "\n",
        "# --- 5. Load TEST data ---\n",
        "print(\"\\nğŸ“¦ Loading TEST data...\")\n",
        "X_img_test = np.load(os.path.join(old_folder, \"X_img_test.npy\"))\n",
        "df_test_clean = pd.read_csv(os.path.join(old_folder, \"df_test_clean.csv\"))\n",
        "\n",
        "txt_test_path = os.path.join(old_folder, \"X_txt_test.npy\")\n",
        "X_txt_test = np.load(txt_test_path) if os.path.exists(txt_test_path) else None\n",
        "\n",
        "assert len(df_test_clean) == len(X_img_test) == len(X_txt_test), \\\n",
        "    \"âŒ Mismatch between test data lengths!\"\n",
        "\n",
        "# --- 6. Save combined files to new directory ---\n",
        "print(\"\\nğŸ’¾ Saving final files...\")\n",
        "\n",
        "np.save(os.path.join(new_folder, \"X_img_train.npy\"), X_img_train)\n",
        "np.save(os.path.join(new_folder, \"X_txt_train.npy\"), X_txt_train)\n",
        "df_train_clean.to_csv(os.path.join(new_folder, \"df_train_clean.csv\"), index=False)\n",
        "\n",
        "np.save(os.path.join(new_folder, \"X_img_test.npy\"), X_img_test)\n",
        "np.save(os.path.join(new_folder, \"X_txt_test.npy\"), X_txt_test)\n",
        "df_test_clean.to_csv(os.path.join(new_folder, \"df_test_clean.csv\"), index=False)\n",
        "\n",
        "print(\"\\nâœ… Final combined files saved successfully!\")\n",
        "\n",
        "print(\"\\n--- Verification ---\")\n",
        "print(f\"Train embeddings: image {X_img_train.shape}, text {X_txt_train.shape}\")\n",
        "print(f\"Test embeddings:  image {X_img_test.shape}, text {X_txt_test.shape}\")\n",
        "print(f\"Train DF: {df_train_clean.shape}\")\n",
        "print(f\"Test DF:  {df_test_clean.shape}\")\n",
        "\n",
        "print(\"\\nğŸ‰ All consolidated embeddings and clean files saved in:\")\n",
        "print(new_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRZzLMf0sIJv",
        "outputId": "ab4d6f75-a740-43f9-d73e-da1f363115cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ğŸ“¦ Loading and combining TRAIN image halves...\n",
            "âœ… Combined train image embeddings: (160186, 512)\n",
            "âœ… Combined train DataFrame: (160186, 16)\n",
            "\n",
            "ğŸ“¦ Loading TRAIN text embeddings...\n",
            "âœ… Found single combined text embedding file.\n",
            "\n",
            "ğŸ“¦ Loading TEST data...\n",
            "\n",
            "ğŸ’¾ Saving final files...\n",
            "\n",
            "âœ… Final combined files saved successfully!\n",
            "\n",
            "--- Verification ---\n",
            "Train embeddings: image (160186, 512), text (160186, 512)\n",
            "Test embeddings:  image (16696, 512), text (16696, 512)\n",
            "Train DF: (160186, 16)\n",
            "Test DF:  (16696, 16)\n",
            "\n",
            "ğŸ‰ All consolidated embeddings and clean files saved in:\n",
            "/content/drive/MyDrive/ML-Project/clip_embeddings_multimodal\n"
          ]
        }
      ]
    }
  ]
}